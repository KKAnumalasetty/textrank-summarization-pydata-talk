{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from math import log\n",
    "import json\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from typing import Any, Optional, Dict, Union, Callable, Tuple, Type, List\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from enum import Enum\n",
    "from operator import itemgetter\n",
    "from functools import singledispatch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by taking a look at a sample of the data (to see how I got this data or to get your own data for your preferred sources and keywords, feel free to use the script I wrote to access news stories through NewsAPI on GitHub). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every January, along with the new year\\xa0come scads of predictions about what lies ahead.', 'This year, device fatigue is rampant, savvy consumers aren\\'t swayed by novelty\\xa0and the so-called\\xa0\"techlash\" against big tech companies like Facebook and Google is in full force.', 'And with\\xa0climate change and increasing natural disasters, people are starting to question the carbon\\xa0footprint of devices and data.', 'Plus, 2020 isn\\'t just \"any\" decade.', 'We have officially entered \"the future,\" a fact made clear when you consider that\\xa0some of the most well-known science fiction movies are now set in the past; the rainy, neon-lit dystopian world of <em So what does that mean for this year?', 'Sure, there will be flashy wearables, new smartphone models\\xa0and efforts to bring virtual reality mainstream.', 'But the real innovations and trends to watch are the ones you might have to look a bit harder to see.', \"Our smartphones don't seem to be going anywhere, but our relationship with them is clearly shifting.\", 'In the last few years, as much as people have talked about technological innovations, <a href=\"https://www.cbc.ca/radio/nowornever/how-to-have-a-healthier-relationship-with-your-phone-1.5338244\" Avery Swartz, author of the upcoming book <em Swartz predicts further changes to hardware and apps, along the lines of Instagram\\'s ongoing <a href=\"https://www.cbc.ca/news/technology/instagram-like-counts-test-in-canada-1.5118501\" Madeline Ashby, a futurist and science fiction novelist, says that in addition to efforts to curb \"digital addiction\" like \"<a href=\"https://www.cbc.ca/news/canada/windsor/millenial-40-day-digital-detox-1.5102618\" Ashby calls this \"ethical consumption\" (noting the term could be contradictory), whereby consumers are making social and political choices with their purchases, or lack thereof.', 'She says the trend will grow as the wealth gap expands and the effects of climate change increase.', \"Indeed, in light of recurring climate catastrophes, there's a sharp and relatively sudden desire on the part of consumers to know more about the carbon footprint of their devices, says Katina Michael, a professor in the School for the Future of Innovation in Society at Arizona State University.\", 'Having witnessed koalas beg humans for water, the sky become red and the sand turn to black ash while returning home to the Australian bushfire crisis over the holidays, she says there is an urgent need for not only better emergency management processes, but also better sustainability\\xa0practices.', '\"Nothing threatens innovation and progress like catastrophe, but nothing propels innovation quite like the loss of human or wildlife,\" Michael says.', '\"When we are without power, access to an automatic teller machine, mobile phone access, or even the internet, we become particularly innovative.\"', 'As for what is (literally) on the horizon in the year ahead, many tech<strong Most satellites hover over the equator, and while \"that\\'s great for TV,\" says Duncan Stewart, director of technology, media &amp; telecommunications research at Deloitte Canada, \"if you want to use a satellite for internet data, that\\'s too far away, which means there will be lag.\"', 'Satellites that are positioned closer to the earth have low\\xa0latency, and constellations formed by hundreds of these LEO satellites are scheduled to have full service in parts of Canada by the end of the year, now that <a href=\"https://www.cbc.ca/news/canada/north/satellite-internet-inuvik-1.5277423\" Nuvujaq Inc.\\xa0develops next generation satellite and \"edge\" computing networks.', 'With edge computing, data storage and processing takes place closer to where it is being used or accessed, to reduce latency.', 'Co-founder Madeline Redfern says that while LEO satellite networks enable high quality internet anywhere on the planet, to date there have been many questions about their financial and technological viability.', 'She expects that\\xa0the true potential of this technology will become more clear in 2020.', '\"In places like Nunavut, LEOs will allow us to do things that the rest of the country takes for granted,\" says Redfern.', '\"Videoconferencing will finally work, enabling friends and family to stay connected and potentially revolutionizing health and education services.\"', \"In 2019, the surging international popularity of the Chinese social media app TikTok signalled a threat to Silicon Valley's supremacy.\", '\"China is shaping the world order in its own image, and it is exporting its technologies and surveillance systems to other countries around the world,\" according to Amy Webb, a professor in the Stern School of Business at New York University and the founder and CEO of the Future Today Institute.', 'China has talked openly about its plans for cyber sovereignty, which is to say, controlling all internet activity within its borders, and \"is quickly ascending to global supremacy,\" says Webb.', 'Webb says that as China expands into African countries, Southeast Asia and Latin America, it will also begin to eschew operating systems, technologies and infrastructure built by the West.', 'She says China has already announced that it will no longer use U.S.-made computers and software.', 'All in all, 2020 is poised to be a big year for tech.', \"But it probably won't be because of a killer app or a shiny new device.\", \"In fact, this year\\xa0you'd be just as well skipping all of the fancy product launches and press releases.\", 'The most exciting stuff is harder to see, but could have lasting impact when it comes to how we relate to technology, to each other and to the world around us.', 'Technology Columnist Ramona Pringle is an associate professor in Faculty of Communication and Design and director of the Creative Innovation Studio at Ryerson University.', 'She is a CBC contributor who writes and reports on the relationship between people and technology.']\n"
     ]
    }
   ],
   "source": [
    "with open('./data/CBC-story-1.json') as f:\n",
    "        data = json.load(f)\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create some tools to help deal with the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best results, we need to filter by part of speech - specifically we filter to get down to the \"open classes\", like verbs, nouns and adjectives (so called because unlike closed classes, like prepositions and articles, new words of these types get added to the vocabulary often). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASCII = re.compile(r\"[^a-z0-9]\")\n",
    "POS = re.compile(r\"^[NJ]\")\n",
    "\n",
    "def filter_pos(token: Dict[str, str]) -> bool:\n",
    "    if not POS.match(token[\"pos\"]) and token[\"pos\"] != \"ADJ\" and token[\"pos\"] != \"CD\":\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the similarity metric that Mihalcea uses in her paper (though you could easily substitute other similarity metrics here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(s1: str, s2: str, **kwargs) -> float:\n",
    "    s1 = set(s1.split())\n",
    "    s2 = set(s2.split())\n",
    "    intersection = len(s1 & s2)\n",
    "    norm = log(len(s1)) + log(len(s2))\n",
    "    return intersection / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function normalizes a sentence by lower casing and removing punctuation from each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_sentence(sent: str) -> str:\n",
    "    tokens = sent.split()\n",
    "    tokens = [norm_token(token) for token in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function normalizes a token by lowercasing and removing non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_token(token: str) -> str:\n",
    "    token = token.lower()\n",
    "    return ASCII.sub(\"\", token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function builds a vocab mapping tokens to indices and produces indices are contiguous and a token has the index of where it first appeared in the tokens list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokens: List[str]) -> Dict[str, int]:\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "    for token in tokens:\n",
    "        vocab[token]\n",
    "    return {k: i for k, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to create the graph! First we create a Vertex object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vertex:\n",
    "    def __init__(self, value: str):\n",
    "        self.value = value\n",
    "        self._edges_out: Dict[int, float] = {}\n",
    "        self._edges_in: Dict[int, float] = {}\n",
    "\n",
    "    @property\n",
    "    def edges_out(self) -> Dict[int, float]:\n",
    "        \"\"\"A mapping of target vertex to weight representing \n",
    "        the edges with this vertex as the source.\"\"\"\n",
    "        return self._edges_out\n",
    "\n",
    "    @property\n",
    "    def edges_in(self) -> Dict[int, float]:\n",
    "        \"\"\"A mapping of source vertex to weight representing \n",
    "        the edges that end at this vertex.\"\"\"\n",
    "        return self._edges_in\n",
    "\n",
    "    @property\n",
    "    def degree_in(self) -> int:\n",
    "        \"\"\"The number of edges that end at this vertex.\"\"\"\n",
    "        return len(self.edges_in)\n",
    "\n",
    "    @property\n",
    "    def degree_out(self) -> int:\n",
    "        \"\"\"The number of edges that start at this vertex.\"\"\"\n",
    "        return len(self.edges_out)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"A summary of this vertex.\"\"\"\n",
    "        return f\"V(term={self.value}, in={self.degree_in}, out={self.degree_out})\"\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        if not isinstance(other, Vertex):\n",
    "            raise TypeError(f\"Can only compare to other Vertex objects, got {type(other)}\")\n",
    "        if self is other:\n",
    "            return True\n",
    "        if self.value != other.value:\n",
    "            return False\n",
    "        if self._edges_out != other._edges_out:\n",
    "            return False\n",
    "        if self._edges_in != other._edges_in:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a Graph object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, vertices: Union[Dict[str, int], List[str]]):\n",
    "        \"\"\"A directed simple graph.\n",
    "        :param vertices: A mapping of vertex labels to integer indices or a \n",
    "        list of vertex labels. If the latter then indices are assigned in order.\n",
    "        \"\"\"\n",
    "        if isinstance(vertices, dict):\n",
    "            if set(vertices.values()) != set(range(len(vertices))):\n",
    "                raise ValueError(\"Vertex indices must be contiguous\")\n",
    "            self.label2idx: Dict[str, int] = vertices\n",
    "        else:\n",
    "            self.label2idx: Dict[str, int] = {n: i for i, n in enumerate(vertices)}\n",
    "        self.idx2label: Dict[int, str] = {i: k for k, i in self.label2idx.items()}\n",
    "\n",
    "    def __getitem__(self, key: Union[str, int]) -> Union[int, str]:\n",
    "        \"\"\"Get either the index or vertex label based on the other one.\n",
    "        :param key: The vertex label or index\n",
    "        :returns: the vertex index of the label is given or the vertex label \n",
    "        if index is given.\n",
    "        \"\"\"\n",
    "        if isinstance(key, int):\n",
    "            return self.idx2label[key]\n",
    "        return self.label2idx[key]\n",
    "\n",
    "    def __contains__(self, key: Union[str, int]) -> bool:\n",
    "        \"\"\"Check if the graph has a vertex labeled key.\n",
    "        :param key: The vertex label or index you are asking about\n",
    "        :returns: True if the vertex exists, False otherwise\n",
    "        \"\"\"\n",
    "        if isinstance(key, int):\n",
    "            return key in self.idx2label\n",
    "        return key in self.label2idx\n",
    "\n",
    "    def add_vertex(self, label: Optional[str]) -> str:\n",
    "        \"\"\"Add a vertex to the graph.\n",
    "        :param label: The label to give the new vertex.\n",
    "        :returns: The vertex label\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _add_vertex(self, label: Optional[str]) -> str:\n",
    "        \"\"\"Add a vertex to the label2idx with a given label or a new one.\n",
    "        :param label: The label for the new vertex\n",
    "        :returns: The label for the new vertex\n",
    "        \"\"\"\n",
    "        if label is None:\n",
    "            label = str(len(self.label2idx))\n",
    "        if label in self.label2idx:\n",
    "            raise ValueError(f\"Node labels must be unique, label {label} is already in use.\")\n",
    "        idx = len(self.label2idx)\n",
    "        self.label2idx[label] = idx\n",
    "        self.idx2label[idx] = label\n",
    "        return idx\n",
    "\n",
    "    def add_edge(self, source: Union[str, int], target: Union[str, int], weight: float = 1.0) -> None:\n",
    "        \"\"\"Add an edge to the graph.\n",
    "        :param source: The vertex label or index of the edge source\n",
    "        :param target: The vertex label or index of the edge target\n",
    "        :param weight: The weight to put on the edge\n",
    "        :raises ValueError: When the source and target node are the same, when the weight is less than zero\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def density(self) -> float:\n",
    "        \"\"\"Get the density of the graph.\n",
    "        The density of a graph is the ratio of edges that the graph has to the number\n",
    "        it could possibly have, this is bounded by 0 and 1.\n",
    "        ```math\n",
    "            D = \\frac{|E|}{|V|(|V| - 1)}\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self.edge_count / (self.vertex_count * (self.vertex_count - 1))\n",
    "\n",
    "    @property\n",
    "    def edge_count(self) -> int:\n",
    "        \"\"\"The number of edges in the graph.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def vertex_count(self) -> int:\n",
    "        \"\"\"The number of vertices in the graph.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"A summary of the graph.\n",
    "        Graph summary includes the number of vertices and edges as well as the\n",
    "        density of the graph.\n",
    "        \"\"\"\n",
    "        return f\"G(V={self.vertex_count}, E={self.edge_count}, D={self.density})\"\n",
    "\n",
    "    def print_graph(self, label_lengths: Optional[int] = None) -> None:\n",
    "        \"\"\"Print the graph is a human readable way.\n",
    "        :param label_length: A cut-off on the length of a single label while printing.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def to_dot(self, directed: bool = False, label_length: Optional[int] = None) -> str:\n",
    "        \"\"\"Get a dot representation of the graph.\n",
    "        The dot graph includes vertex labels and edge weights.\n",
    "        :param directed: Should the dot representation be directed of not. Most graphs created\n",
    "            in the package are directed but have the same weight in either direction so we\n",
    "            can collapse the graph into an undirected weighted graph for cleaner plotting.\n",
    "            Note: Collapsing this doesn't check that the weights in each direction are the same,\n",
    "            it just plots a single edge.\n",
    "        :param label_length: A cut-off on the length allowed for a single label in the printing.\n",
    "        :returns: The representations of the graph as a dot string.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyList(Graph):\n",
    "    def __init__(self, vertices: Dict[str, int]):\n",
    "        super().__init__(vertices)\n",
    "        self._vertices: List[Vertex] = [Vertex(l) for l in self.label2idx]\n",
    "\n",
    "    @property\n",
    "    def vertices(self) -> List[Vertex]:\n",
    "        \"\"\"The vertices in this graph.\"\"\"\n",
    "        return self._vertices\n",
    "\n",
    "    def add_vertex(self, label: Optional[str]) -> int:\n",
    "        \"\"\"Add a vertex to the graph.\n",
    "        :param label: The label to give the new vertex.\n",
    "        :returns: The vertex index\n",
    "        \"\"\"\n",
    "        idx = self._add_vertex(label)\n",
    "        if idx != len(self.vertices):\n",
    "            raise ValueError(\n",
    "                \"The added vertex has a label that is out of order, expected: {len(self.vertices)} found: {idx}\"\n",
    "            )\n",
    "        self.vertices.append(Vertex(label))\n",
    "        return idx\n",
    "\n",
    "    def add_edge(self, source: Union[str, int], target: Union[str, int], weight: float = 1.0) -> None:\n",
    "        \"\"\"Add an edge to the graph.\n",
    "        :param source: The vertex label or index of the edge source\n",
    "        :param target: The vertex label or index of the edge target\n",
    "        :param weight: The weight to put on the edge\n",
    "        :raises ValueError: When the source and target node are the same, when the weight is less than zero\n",
    "        \"\"\"\n",
    "        if weight < 0.0:\n",
    "            raise ValueError(f\"Edge weight must be greater than zero, got {weight}\")\n",
    "        source_idx = source if isinstance(source, int) else self[source]\n",
    "        target_idx = target if isinstance(target, int) else self[target]\n",
    "        if source_idx == target_idx:\n",
    "            raise ValueError(f\"Self loops are not allowed, found edge with source and target if {source_idx}\")\n",
    "        source_vertex = self.vertices[source_idx]\n",
    "        target_vertex = self.vertices[target_idx]\n",
    "        source_vertex.edges_out[target_idx] = weight\n",
    "        target_vertex.edges_in[source_idx] = weight\n",
    "\n",
    "    @property\n",
    "    def vertex_count(self) -> int:\n",
    "        \"\"\"The number of vertices in the graph.\"\"\"\n",
    "        return len(self.vertices)\n",
    "\n",
    "    @property\n",
    "    def edge_count(self) -> int:\n",
    "        \"\"\"The number of edges in the graph.\"\"\"\n",
    "        return sum(v.degree_out for v in self.vertices)\n",
    "\n",
    "    def print_graph(self, label_length: Optional[int] = None) -> None:\n",
    "        \"\"\"Print the graph is a human readable way.\n",
    "        :param label_length: A cut-off on the length of a single label while printing.\n",
    "        \"\"\"\n",
    "        print(str(self))\n",
    "        for v in self.vertices:\n",
    "            print(f\"\\tVertex {self[v.value]}: {v.value[:label_length]}\")\n",
    "            print(f\"\\t\\tOutbound:\")\n",
    "            for idx, weight in v.edges_out.items():\n",
    "                print(f\"\\t\\t\\t{self[v.value]} -> {idx}: {weight}\")\n",
    "            print(f\"\\t\\tInbound:\")\n",
    "            for idx, weight in v.edges_in.items():\n",
    "                print(f\"\\t\\t\\t{self[v.value]} <- {idx}: {weight}\")\n",
    "\n",
    "    def to_dot(self, directed: bool = False, label_length: Optional[int] = None) -> str:\n",
    "        \"\"\"Get a dot representation of the graph.\n",
    "        The dot graph includes vertex labels and edge weights.\n",
    "        :param directed: Should the dot representation be directed of not. Most graphs created\n",
    "            in the package are directed but have the same weight in either direction so we\n",
    "            can collapse the graph into an undirected weighted graph for cleaner plotting.\n",
    "            Note: Collapsing this doesn't check that the weights in each direction are the same,\n",
    "            it just plots a single edge.\n",
    "        :param label_length: A cut-off on the length allowed for a single label in the printing.\n",
    "        :returns: The representations of the graph as a dot string.\n",
    "        \"\"\"\n",
    "        if directed:\n",
    "            return self._to_directed_dot(label_length)\n",
    "        return self._to_undirected_dot(label_length)\n",
    "\n",
    "    def _to_directed_dot(self, label_length: Optional[int] = None) -> str:\n",
    "        \"\"\"Get a dot representation of the graph as a directed graph.\n",
    "        :param label_length: A cut-off on the length allowed for a single label in the printing.\n",
    "        :returns: The representations of the graph as a dot string.\n",
    "        \"\"\"\n",
    "        dot = [\"digraph G {\"]\n",
    "        for v in self.vertices:\n",
    "            dot.append(f'\\t{self[v.value]} [label=\"{v.value[:label_length]}\"];')\n",
    "            for idx, weight in v.edges_out.items():\n",
    "                dot.append(f'\\t{self[v.value]} -> {idx} [label=\"{weight}\"];')\n",
    "        dot.append(\"}\")\n",
    "        return \"\\n\".join(dot)\n",
    "\n",
    "    def _to_undirected_dot(self, label_length: Optional[int] = None) -> str:\n",
    "        \"\"\"Get a dot representation of the graph as a undirected graph.\n",
    "        Note:\n",
    "            This doesn't check that graph edges can actually be collapsed into a single edge.\n",
    "        :param label_length: A cut-off on the length allowed for a single label in the printing.\n",
    "        :returns: The representations of the graph as a dot string.\n",
    "        \"\"\"\n",
    "        dot = [\"graph G {\"]\n",
    "        edges = set()\n",
    "        for v in self.vertices:\n",
    "            dot.append(f'\\t{self[v.value]} [label=\"{v.value[:label_length]}\"];')\n",
    "            for idx, weight in v.edges_out.items():\n",
    "                if (self[v.value], idx) in edges or (idx, self[v.value]) in edges:\n",
    "                    continue\n",
    "                dot.append(f'\\t{self[v.value]} -- {idx} [label=\"{weight}\"];')\n",
    "                edges.add((self[v.value], idx))\n",
    "        dot.append(\"}\")\n",
    "        return \"\\n\".join(dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we get into TextRank! This first function calculates the total weight for a collection of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvergenceType = Enum(\"ConvergenceType\", \"ALL ANY\")\n",
    "def sum_edges(edges: Dict[str, float]) -> float:\n",
    "    return sum(edges.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function accumulates the scores from all nodes that have incoming connections to a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_score(vertex: Vertex, ws: List[float], denom: List[float]):\n",
    "    return math.fsum([weight / denom[edge] * ws[edge] for edge, weight in vertex.edges_in.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates the initial scores for each node and pre-computes the outgoing strength for the Adjacency List graph. The sum of the weights for outbound edges for a given node doesn't change as text rank runs because it is based only on the values in the graph, not on ws for the node so we can pre-compute and reuse it instead of always recalculating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank_init(graph: Graph, uniform: bool = False, seed: Optional[int] = None) -> Tuple[List[float], List[float]]:\n",
    "    raise NotImplementedError\n",
    "\n",
    "def text_rank_init_list(\n",
    "    graph: AdjacencyList, uniform: bool = True, seed: Optional[int] = None\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    random.seed(seed)\n",
    "    denom = [sum_edges(v.edges_out) for v in graph.vertices]\n",
    "    # If the sum off all outgoing edges of V_j is 0.0 then the incoming edge from V_j to V_i will be 0.0\n",
    "    # We can use anything as the denominator and the value will still be zero\n",
    "    denom = [d if d != 0.0 else 1.0 for d in denom]\n",
    "    if uniform:\n",
    "        ws = [1 / len(graph.vertices) for _ in graph.vertices]\n",
    "    else:\n",
    "        ws = [random.random() for _ in graph.vertices]\n",
    "        norm = sum(ws)\n",
    "        ws = [w / norm for w in ws]\n",
    "    return ws, denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the new score for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank_update_list(\n",
    "    graph: AdjacencyList, ws: List[float], denom: List[float], dampening: float = 0.85\n",
    ") -> List[float]:\n",
    "    updates = [accumulate_score(v, ws, denom) for v in graph.vertices]\n",
    "    ws = [(1 - dampening) + dampening * update for update in updates]\n",
    "    return ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function outputs the TextRank score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singledispatch\n",
    "def text_rank_output(graph: Graph, ws: List[float]) -> List[Tuple[str, float]]:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "@text_rank_output.register(AdjacencyList)\n",
    "def text_rank_output_list(graph: AdjacencyList, ws: List[float]) -> List[Tuple[str, float]]:\n",
    "    norm = sum(ws)\n",
    "    ws = [w / norm for w in ws]\n",
    "    return sorted(zip(map(lambda v: v.value, graph.vertices), ws), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function runs TextRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank(\n",
    "    graph: Graph,\n",
    "    dampening: float = 0.85,\n",
    "    convergence: float = 0.0001,\n",
    "    convergence_type: ConvergenceType = ConvergenceType.ALL,\n",
    "    niter: int = 200,\n",
    "    uniform: bool = False,\n",
    "    seed: Optional[int] = None,\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"Implementation of text rank from here https://www.aclweb.org/anthology/W04-3252.pdf\n",
    "    :param graph: The graph we are running text rank on\n",
    "    :param dampening: A scalar between 0 and 1. Used to simulate randomly jumping from one vertex to another.\n",
    "    :param convergence: An early stopping criteria, when any or all of the node scores change by less than `convergence`\n",
    "        we stop updating the graph. Set to `0` to turn off early stopping.\n",
    "    :param convergence_type: Should we stop when all nodes move less than `convergence` or when a single node does\n",
    "    :param niter: An upper bound on the number of iterations to run\n",
    "    :param uniform: Should we initialize state vector to have equal prob for each node?\n",
    "    :param seed: A reproducability seed to initialization of the node scores.\n",
    "    :returns: Pairs of (node label, scores) sorted by score\n",
    "    \"\"\"\n",
    "    if not 0 <= dampening <= 1:\n",
    "        raise ValueError(f\"dampening must be between `0` and `1`, got {dampening}\")\n",
    "    converge = all if convergence_type is ConvergenceType.ALL else any\n",
    "\n",
    "    ws_prev, denom = text_rank_init(graph, uniform=uniform, seed=seed)\n",
    "\n",
    "    for _ in range(niter):\n",
    "        ws = text_rank_update(graph, ws_prev, denom, dampening)\n",
    "        if converge(abs(p - c) < convergence for p, c in zip(ws_prev, ws)):\n",
    "            break\n",
    "        ws_prev = ws\n",
    "\n",
    "    return text_rank_output(graph, ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_graph(\n",
    "    sentences: List[str],\n",
    "    sim: Callable[..., float] = overlap,\n",
    "    norm: Callable[[str], str] = norm_sentence,\n",
    "    GraphType: Type[Graph] = AdjacencyList,\n",
    ") -> Tuple[Graph, Dict[str, List[int]]]:\n",
    "    \"\"\"Generate a fully connected graph with edges between all sentences.\n",
    "    Note:\n",
    "        This also generates a dict mapping normalized vertex labels to their offsets in the original\n",
    "        data. This can be used to run text rank on normalized data but return the original strings.\n",
    "        You can also sort the output by offsets to make it maybe more readable?\n",
    "    :param sentences: The sentences to summarize.\n",
    "    :param sim: A callable that returns the similarity between two vertices, used to set the weight of the edge.\n",
    "        The callable should have a signature like:\n",
    "            sim(\n",
    "                normed_s1,\n",
    "                normed_s2,\n",
    "                raw_s1=raw_s1,\n",
    "                raw_s2=raw_s2,\n",
    "                s1_idx=s1_idx,\n",
    "                s2_idx=s2_idx,\n",
    "            ) -> float:\n",
    "        Where normed_s1/2 is the normalized strings of the two sentences, raw_s1/2 is the version of the sentence\n",
    "        before getting normalized and s1/2_idx is the index of the sentences in the token list. This should\n",
    "        facilitate both simple and complex similarity functions and also experiments that the actual flow of text\n",
    "        to determine connections.\n",
    "    :param norm: A function the returns a normalized version of the input sentence. Default implementation lowercases\n",
    "        string and removes non alpha-numeric characters.\n",
    "        This is used so simple similarity functions like the set overlap in the paper work well.\n",
    "    :param GraphType: The Graph class to use.\n",
    "    :returns: The constructed graph and offsets mapping normalized vertex labels to their place in the original text.\n",
    "    \"\"\"\n",
    "    offsets = defaultdict(list)\n",
    "    normed = [norm(sentence) for sentence in sentences]\n",
    "    for i, norm in enumerate(normed):\n",
    "        offsets[norm].append(i)\n",
    "\n",
    "    vocab = build_vocab(normed)\n",
    "    graph = GraphType(vocab)\n",
    "\n",
    "    for (i, src), (j, tgt) in combinations(enumerate(normed), 2):\n",
    "        graph.add_edge(src, tgt, sim(src, tgt, raw_s1=sentences[i], raw_s2=sentences[j], s1_idx=i, s2_idx=j))\n",
    "        graph.add_edge(tgt, src, sim(tgt, src, raw_s1=sentences[j], raw_s2=sentences[i], s1_idx=j, s2_idx=i))\n",
    "\n",
    "    return graph, offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, our summarize function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(\n",
    "    sentences: List[str],\n",
    "    nsents: Optional[int] = None,\n",
    "    keep_order: bool = True,\n",
    "    dampening: float = 0.85,\n",
    "    convergence: float = 0.0001,\n",
    "    convergence_type: ConvergenceType = ConvergenceType.ALL,\n",
    "    niter: int = 200,\n",
    "    seed: Optional[int] = None,\n",
    "    sim: Callable[..., float] = overlap,\n",
    "    norm: Callable[[str], str] = norm_sentence,\n",
    "    GraphType: Type[Graph] = AdjacencyList,\n",
    ") -> List[str]:\n",
    "   \n",
    "    graph, offsets = sentence_graph(sentences, sim, norm, GraphType)\n",
    "    if nsents is None:\n",
    "        nsents = len(sentences) // 3\n",
    "    selected = text_rank(\n",
    "        graph, dampening=dampening, convergence=convergence, convergence_type=convergence_type, niter=niter, seed=seed,\n",
    "    )[:nsents]\n",
    "    indices = [offsets[s[0]][0] for s in selected]\n",
    "    if keep_order:\n",
    "        return [sentences[i] for i in sorted(indices)]\n",
    "    return [sentences[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-cbed1950ea3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msents\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msumms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-b86d38a4ad86>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(sentences, nsents, keep_order, dampening, convergence, convergence_type, niter, seed, sim, norm, GraphType)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnsents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     selected = text_rank(\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvergence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvergence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvergence_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvergence_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mniter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     )[:nsents]\n\u001b[1;32m     21\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-164c03fa99aa>\u001b[0m in \u001b[0;36mtext_rank\u001b[0;34m(graph, dampening, convergence, convergence_type, niter, uniform, seed)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mconverge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconvergence_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mConvergenceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mws_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_rank_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-8487befd475f>\u001b[0m in \u001b[0;36mtext_rank_init\u001b[0;34m(graph, uniform, seed)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_rank_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m def text_rank_init_list(\n\u001b[1;32m      5\u001b[0m     \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdjacencyList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "summs = []\n",
    "sents = []\n",
    "sentences = []\n",
    "for file_name in Path('./data').glob('*.json'):\n",
    "    with open(file_name) as f:\n",
    "        sentences.append(json.load(f))\n",
    "print(sents[0:1])\n",
    "for sents in sentences:\n",
    "    summs.append(summarize(sents, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for summ in summs:\n",
    "    print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
